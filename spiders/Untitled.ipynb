{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4502b396",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-14 05:06:13 [scrapy.utils.log] INFO: Scrapy 2.6.2 started (bot: scrapybot)\n",
      "2022-09-14 05:06:13 [scrapy.utils.log] INFO: Versions: lxml 4.8.0.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.5 (default, May 18 2021, 14:42:02) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 20.0.1 (OpenSSL 1.1.1q  5 Jul 2022), cryptography 3.4.7, Platform Windows-10-10.0.22000-SP0\n",
      "2022-09-14 05:06:13 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2022-09-14 05:06:13 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2022-09-14 05:06:13 [scrapy.extensions.telnet] INFO: Telnet Password: 157358916617cee6\n",
      "2022-09-14 05:06:13 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-09-14 05:06:13 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-09-14 05:06:13 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-09-14 05:06:13 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-09-14 05:06:13 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-09-14 05:06:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-09-14 05:06:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-09-14 05:06:16 [filelock] DEBUG: Attempting to acquire lock 2115599460672 on C:\\Users\\diego\\anaconda3\\envs\\env1_cloned_tf\\lib\\site-packages\\tldextract\\.suffix_cache/publicsuffix.org-tlds\\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
      "2022-09-14 05:06:16 [filelock] DEBUG: Lock 2115599460672 acquired on C:\\Users\\diego\\anaconda3\\envs\\env1_cloned_tf\\lib\\site-packages\\tldextract\\.suffix_cache/publicsuffix.org-tlds\\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
      "2022-09-14 05:06:16 [filelock] DEBUG: Attempting to release lock 2115599460672 on C:\\Users\\diego\\anaconda3\\envs\\env1_cloned_tf\\lib\\site-packages\\tldextract\\.suffix_cache/publicsuffix.org-tlds\\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
      "2022-09-14 05:06:16 [filelock] DEBUG: Lock 2115599460672 released on C:\\Users\\diego\\anaconda3\\envs\\env1_cloned_tf\\lib\\site-packages\\tldextract\\.suffix_cache/publicsuffix.org-tlds\\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
      "2022-09-14 05:06:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.amazon.com/s?k=data+science&s=relevanceblender&crid=2YKZLS0CJFDGI&qid=1663122382&sprefix=data+science%2Caps%2C828/> (referer: None)\n",
      "2022-09-14 05:06:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.amazon.com/s?k=data+science&s=relevanceblender&crid=2YKZLS0CJFDGI&qid=1663122382&sprefix=data+science%2Caps%2C828/> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\diego\\anaconda3\\envs\\env1_cloned_tf\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 132, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"C:\\Users\\diego\\anaconda3\\envs\\env1_cloned_tf\\lib\\site-packages\\scrapy\\utils\\python.py\", line 354, in __next__\n",
      "    return next(self.data)\n",
      "  File \"C:\\Users\\diego\\anaconda3\\envs\\env1_cloned_tf\\lib\\site-packages\\scrapy\\utils\\python.py\", line 354, in __next__\n",
      "    return next(self.data)\n",
      "  File \"C:\\Users\\diego\\anaconda3\\envs\\env1_cloned_tf\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 66, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\diego\\anaconda3\\envs\\env1_cloned_tf\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 29, in process_spider_output\n",
      "    for x in result:\n",
      "  File \"C:\\Users\\diego\\anaconda3\\envs\\env1_cloned_tf\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 66, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\diego\\anaconda3\\envs\\env1_cloned_tf\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 342, in <genexpr>\n",
      "    return (_set_referer(r) for r in result or ())\n",
      "  File \"C:\\Users\\diego\\anaconda3\\envs\\env1_cloned_tf\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 66, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\diego\\anaconda3\\envs\\env1_cloned_tf\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 40, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"C:\\Users\\diego\\anaconda3\\envs\\env1_cloned_tf\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 66, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\diego\\anaconda3\\envs\\env1_cloned_tf\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 58, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"C:\\Users\\diego\\anaconda3\\envs\\env1_cloned_tf\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 66, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"<ipython-input-1-3f2592338a2d>\", line 27, in parse\n",
      "    self.list_items.append(items)\n",
      "NameError: name 'items' is not defined\n",
      "2022-09-14 05:06:16 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-09-14 05:06:16 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 337,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 132360,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 2.0206,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 9, 14, 3, 6, 16, 225491),\n",
      " 'httpcompression/response_bytes': 804056,\n",
      " 'httpcompression/response_count': 1,\n",
      " 'log_count/DEBUG': 6,\n",
      " 'log_count/ERROR': 1,\n",
      " 'log_count/INFO': 10,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'spider_exceptions/NameError': 1,\n",
      " 'start_time': datetime.datetime(2022, 9, 14, 3, 6, 14, 204891)}\n",
      "2022-09-14 05:06:16 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "import csv\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import pandas as pd\n",
    "class FirstWebsiteSpider(scrapy.Spider):\n",
    "    name = 'first_website'\n",
    "    #https://www.amazon.com/s?k=data+science&i=stripbooks-intl-ship&page=3&crid=1YWU6OJK4L3BA&qid=1662895059&sprefix=data+science%2Cstripbooks-intl-ship%2C247&ref=sr_pg_3\n",
    "    allowed_domains = [r\"www.amazon.com\"]\n",
    "    start_urls = [r\"https://www.amazon.com/s?k=data+science&s=relevanceblender&crid=2YKZLS0CJFDGI&qid=1663122382&sprefix=data+science%2Caps%2C828/\"]\n",
    "    list_items=[]\n",
    "    j=2 \n",
    "    x=pd.DataFrame([])\n",
    "    n_page_to_scrape=6\n",
    "    def parse(self, response):\n",
    "        title=response.xpath(\"//a[@class='a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal']/span/text()\").getall()\n",
    "        authors=[]\n",
    "        dates=[]\n",
    "        #self.j=j\n",
    "        box=response.xpath(\"//div[@class='a-row a-size-base a-color-secondary']/div[@class='a-row']\")\n",
    "        for i in box:\n",
    "            date=i.xpath(\".//span[@class='a-size-base a-color-secondary a-text-normal']/text()\").getall()\n",
    "            author=i.xpath(\".//a[@class='a-size-base a-link-normal s-underline-text s-underline-link-text s-link-style']/text() | (.//span[@class='a-size-base']/text())\").getall()\n",
    "            authors.append(author)   \n",
    "            dates.append(date)\n",
    "        \n",
    "        #items={'titles':title,'authors':authors,'dates':dates}\n",
    "        self.list_items.append(items)     \n",
    "        #url_link_page=f\"/s?k=data+science&amp;i=stripbooks-intl-ship&amp;page={self.j}&amp;crid=20YNDMUXOLCJ6&amp;qid=1663034092&amp;sprefix=data+science%2Cstripbooks-intl-ship%2C314&amp;ref=sr_pg_{self.j}\"\n",
    "         #pd.DataFrame([title,authors], columns=['titles','authors'])\n",
    "        pagination=response.xpath(\"//div[@class='a-section a-text-center s-pagination-container']/span[@class='s-pagination-strip']\")\n",
    "        next_page_url=pagination.xpath(\"//span[@class='s-pagination-strip']/a[last()]/@href\").get()\n",
    "        \n",
    "        #for z in next_page_url:\n",
    "            #if z==url_link_page:\n",
    "        #if self.j<5:\n",
    "            #self.j+=1\n",
    "        if next_page_url and self.j<self.n_page_to_scrape:\n",
    "            print(self.j)\n",
    "            self.j+=1\n",
    "            yield response.follow(url=next_page_url, callback=self.parse)#,self.j  \n",
    "        self.x=pd.DataFrame(self.items, columns=['titles','authors','dates'])\n",
    "\n",
    "        \n",
    "        #yield (items,len(authors),len(dates))\n",
    "        #yield \n",
    "        if self.j==self.n_page_to_scrape:\n",
    "            yield self.x.to_csv(\"outputfile.csv\",sep=\",\")\n",
    "            #with open(\"outputfile.csv\", \"w\", newline=\"\") as f:\n",
    "                #writer = csv.DictWriter(f, ['titles','authors','dates'])\n",
    "                #writer.writeheader()           \n",
    "                #writer.writerows(self.list_items)\n",
    "                #print(self.list_items) #\n",
    "#//div[@class='a-row']/a[@class='a-size-base a-link-normal s-underline-text s-underline-link-text s-link-style']/text() | ( //div[@class='a-#row']/span[@class='a-size-base']/text()\n",
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "})\n",
    "\n",
    "process.crawl(FirstWebsiteSpider)\n",
    "process.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e13533af",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-38d765480056>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mauthors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtitles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFirstWebsiteSpider\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscrapy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSpider\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'response' is not defined"
     ]
    }
   ],
   "source": [
    "authors,titles=FirstWebsiteSpider.parse(scrapy.Spider,response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08551c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "@href='/s?k=data+science&amp;i=stripbooks-intl-ship&amp;page=3&amp;crid=20YNDMUXOLCJ6&amp;qid=1663030048&amp;sprefix=data+science%2Cstripbooks-intl-ship%2C314&amp;ref=sr_pg_3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "004de66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "j=1\n",
    "p=f'/s?k=data+science&amp;i=stripbooks-intl-ship&amp;page={j}&amp;crid=20YNDMUXOLCJ6&amp;qid=1663030048&amp;sprefix=data+science%2Cstripbooks-intl-ship%2C314&amp;ref=sr_pg_{j}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e413f5a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/s?k=data+science&amp;i=stripbooks-intl-ship&amp;page=1&amp;crid=20YNDMUXOLCJ6&amp;qid=1663030048&amp;sprefix=data+science%2Cstripbooks-intl-ship%2C314&amp;ref=sr_pg_1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e7e5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "<a href=>3</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
